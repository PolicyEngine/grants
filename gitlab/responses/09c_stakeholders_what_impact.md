PolicyEngine is infrastructure, so we measure direct technical improvements while partners measure downstream user impact.

Our technical deliverables include Atlas preventing link rot for program documents across 50 states, encoding 5-10 programs nationwide using AI (LIHEAP, WIC, Section 8, rental assistance), reducing encoding time by 50% or more, publishing quality metrics comparing AI-generated code against human golden PR baselines, and publishing research on LLM benefit estimation accuracy with versus without structured tools.

Partners will measure user impact. MyFriendBen, Amplifi, Starlight, Student Basic Needs Coalition, and Mirza will track user engagement with newly encoded programs, completion rates, and benefits accessed. We'll collect and publish aggregated partner metrics showing how infrastructure improvements translate to user outcomes.

The research component demonstrates the value of structured tools versus unstructured AI. Our published evaluation will establish which LLMs work reliably for benefit calculations, where they fail dangerously (hallucinating eligibility or inventing rules), and why tools like PolicyEngine matter compared to telling people to "just ask ChatGPT."

The open source multiplier is significant: any improvements to PolicyEngine's Python package and Docker image are freely available to unlimited future users. We can't measure everyone who benefits—government agencies, researchers, and civic tech organizations we don't know about yet—because that's the nature of public infrastructure. Anyone can use it without telling us.
